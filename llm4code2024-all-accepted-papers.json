[
    {
        "pid": 5,
        "title": "Applying Large Language Models to Enhance the Assessment of Parallel Functional Programming Assignments",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-8051366c2c61ffca0cbb61de929a9092affe5d91e8628b23c8c1d6c5fdcdcae4",
            "timestamp": 1702008096,
            "size": 613848,
            "pages": 8
        },
        "abstract": "Courses in computer science often assess student programming assignments manually, with the intent of providing in-depth feedback to each student regarding correctness, style, efficiency, and other quality attributes. As class sizes increase, however, it is hard to provide consistent detailed feedback.  Large language models (LLMs), such as ChatGPT, offer a promising alternative to help automate this process in a consistent, scalable, and minimally-biased manner.\r\n  \r\nThis paper explores ChatGPT-4's scalablility and accuracy in assessing parallel functional programming assignments based on predefined rubrics. We employ a method that compares assessments generated by ChatGPT against human graders to analyze accuracy, precision, and recall in identifying programming mistakes. Our results show that LLMs can improve objectivity and grading efficiency, thereby acting as a complementary tool to human graders for computer science graduate and undergraduate students.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "skyler.h.grandel@vanderbilt.edu",
                "first": "Skyler",
                "last": "Grandel",
                "affiliation": "Vanderbilt University",
                "contact": true
            },
            {
                "email": "d.schmidt@vanderbilt.edu",
                "first": "Douglas",
                "last": "Schmidt",
                "affiliation": "Vanderbilt University, USA"
            },
            {
                "email": "kevin.leach@vanderbilt.edu",
                "first": "Kevin",
                "last": "Leach",
                "affiliation": "Vanderbilt University",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "tzimmer@microsoft.com": "collaborator"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702008096
    },
    {
        "pid": 7,
        "title": "MoonBit: Explore the Design of an AI-Friendly Programming Language",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-3de38c84d6c0fc1087963b7764428e4c21bb62d8bed2caea82ed8b166398a674",
            "timestamp": 1702035482,
            "size": 481392,
            "pages": 4
        },
        "abstract": "MoonBit, a new general-purpose programming language designed for cloud and\r\n  edge computing, was initiated in late 2022, coinciding with the announcement\r\n  of ChatGPT. Language models like GPT, capable of producing practical programs,\r\n  are revolutionizing the way we write programs and interact with computers.\r\n  However, significant challenges persist, such as the models' inability to\r\n  understand the global context of a whole project with its dependencies, the\r\n  need for human verification and correction of generated code, and the lack of\r\n  assurance in meeting basic requirements like syntactic correctness.\r\n\r\n  In this paper, we explore the design of the MoonBit language highlighting its\r\n  AI integration, emphasizing the synergy between traditional code intelligence\r\n  and large language model capabilities. We also introduce a real-time,\r\n  semantics-based sampler to guide the inference process of language models.\r\n  This approach ensures the generated programs are both syntactically correct\r\n  and free from obvious semantic flaws, such as type errors. Crucially, this has\r\n  been achieved with minimal impact on overall performance. Our evaluation\r\n  demonstrates a notable improvement in code quality, achieved without\r\n  sacrificing the models' responsiveness.",
        "paper_type": "2. Position Paper",
        "authors": [
            {
                "email": "feihaoxiang@idea.edu.cn",
                "first": "Haoxiang",
                "last": "Fei",
                "affiliation": "International Digital Economy Academy",
                "contact": true
            },
            {
                "email": "zhangyu@idea.edu.cn",
                "first": "Yu",
                "last": "Zhang",
                "affiliation": "International Digital Economy Academy",
                "contact": true
            },
            {
                "email": "bob.hongbo.zhang@idea.edu.cn",
                "first": "Hongbo",
                "last": "Zhang",
                "affiliation": "International Digital Economy Academy",
                "contact": true
            },
            {
                "email": "wangylin36@mail.sysu.edu.cn",
                "first": "Yanlin",
                "last": "Wang",
                "affiliation": "Sun Yat-sen University",
                "contact": true
            },
            {
                "email": "liuqing@idea.edu.cn",
                "first": "Qing",
                "last": "Liu",
                "affiliation": "International Digital Economy Academy",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "wangylin36@mail.sysu.edu.cn": "author"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702025338
    },
    {
        "pid": 10,
        "title": "Industrial Experience Report on AI-Assisted Coding in Professional Software Development",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-e74958cdedf6b086830d5424fc9526917f98c644468e80e25636c9b2ae019a32",
            "timestamp": 1702044563,
            "size": 499250,
            "pages": 6
        },
        "abstract": "AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are a minor concern although awareness exists.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "rudolf.ramler@scch.at",
                "first": "Rudolf",
                "last": "Ramler",
                "affiliation": "Software Competence Center Hagenberg GmbH",
                "contact": true
            },
            {
                "email": "lukas.fischer@scch.at",
                "first": "Lukas",
                "last": "Fischer",
                "affiliation": "Software Competence Center Hagenberg GmbH"
            },
            {
                "email": "michael.moser@scch.at",
                "first": "Michael",
                "last": "Moser",
                "affiliation": "Software Competence Center Hagenberg"
            },
            {
                "email": "markus.nissl@bds421.com",
                "first": "Markus",
                "last": "Nissl",
                "affiliation": "Building Digital Solutions 421 GmbH"
            },
            {
                "email": "rene.heinzl@bds421.com",
                "first": "Rene",
                "last": "Heinzl",
                "affiliation": "Building Digital Solutions 421 GmbH"
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702044563,
        "final_submitted": true,
        "final_submitted_at": 1705667179
    },
    {
        "pid": 13,
        "title": "Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-46ebf2666fb15d351b5b3d2da56fce203da12f6825f759179a590d3e0630c902",
            "timestamp": 1701961827,
            "size": 683370,
            "pages": 22
        },
        "abstract": "A significant amount of research is focused on developing and evaluating\r\nlarge language models for a variety of code synthesis tasks. These include\r\nsynthesizing code from natural language instructions, synthesizing tests from\r\ncode, and synthesizing explanations of code. In contrast, the behavior of \r\ninstructional code editing with LLMs is understudied. These are tasks in which the model is\r\ninstructed to update a block of code provided in a prompt. The editing \r\ninstruction may ask for a feature to added or removed, describe a bug and ask\r\nfor a fix, ask for a different kind of solution, or many other common code editing\r\ntasks.\r\n\r\nWe introduce a carefully crafted benchmark of code editing tasks and use it\r\nevaluate several cutting edge LLMs. Our evaluation exposes a significant gap\r\nbetween the capabilities of state-of-the-art open and closed models. For\r\nexample, even GPT-3.5-Turbo is 8.8\\% better than the best open model at\r\nediting code. \r\n\r\nWe also introduce a new, carefully curated, permissively licensed training set of code edits\r\ncoupled with natural language instructions.\r\nUsing this training set, we show that we can fine-tune open Code LLMs to significantly\r\nimprove their code editing capabilities.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "cassano.f@northeastern.edu",
                "first": "Federico",
                "last": "Cassano",
                "affiliation": "Northeastern University",
                "contact": true
            },
            {
                "email": "li.tao@northeastern.edu",
                "first": "Luisa",
                "last": "Li",
                "affiliation": "Northeastern University",
                "contact": true
            },
            {
                "email": "sethi.ak@northeastern.edu",
                "first": "Akul",
                "last": "Sethi",
                "affiliation": "Northeastern University",
                "contact": true
            },
            {
                "email": "shinn.n@northeastern.edu",
                "first": "Noah",
                "last": "Shinn",
                "affiliation": "Northeastern University"
            },
            {
                "email": "ab129@wellesley.edu",
                "first": "Abby",
                "last": "Brennan-Jones",
                "affiliation": "Wellesley College"
            },
            {
                "email": "anton@huggingface.co",
                "first": "Anton",
                "last": "Lozhkov",
                "affiliation": "Hugging Face"
            },
            {
                "email": "carolyn.anderson@wellesley.edu",
                "first": "Carolyn",
                "last": "Anderson",
                "affiliation": "Wellesley College",
                "contact": true
            },
            {
                "email": "a.guha@northeastern.edu",
                "first": "Arjun",
                "last": "Guha",
                "affiliation": "Northeastern University and Roblox",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "2. Non-archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701961946
    },
    {
        "pid": 18,
        "title": "An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-7056ac562856cfe4299b504b574769472ca7374110730251629e5f1f1ec2bc53",
            "timestamp": 1702030419,
            "size": 1033356,
            "pages": 8
        },
        "abstract": "Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in large-scale software engineering development have not been fully explored yet. In this study we explore the usefulness of LLMs for 214 students working on an academic software engineering project in teams consisting of up to six members. The students were encouraged to integrate LLMs into their development tool-chain.\r\n\r\nIn this paper, we analyze statistics for the AI generated code, prompts used for code generation and human intervention levels to integrate the code. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook from a student perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "sanka@nus.edu.sg",
                "first": "Sanka",
                "last": "Rasnayaka",
                "affiliation": "National University of Singapore",
                "contact": true
            },
            {
                "email": "wguanlin@u.nus.edu.sg",
                "first": "Wang",
                "last": "Guanlin",
                "affiliation": "National University of Singapore",
                "contact": true
            },
            {
                "email": "ridwan@comp.nus.edu.sg",
                "first": "Ridwan",
                "last": "Shariffdeen",
                "affiliation": "National University of Singapore",
                "contact": true
            },
            {
                "email": "gni@nus.edu.sg",
                "first": "Ganesh Neelakanta",
                "last": "Iyer",
                "affiliation": "National University of Singapore",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "shinhwei.tan@concordia.ca": "collaborator"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702030419
    },
    {
        "pid": 19,
        "title": "Investigating the Proficiency of Large Language Models in Formative Feedback Generation for Student Programmers",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-050da6e75543367277959fd8d940cf7c1c373b10ca91f6b1cf7f13da1f3f2ab5",
            "timestamp": 1702069032,
            "size": 446683,
            "pages": 6
        },
        "abstract": "Generative AI has considerably altered traditional workplace practice across numerous industries. Ever since the emergence of large\r\nlanguage models (LLMs), their potential to generate formative feedback for introductory programming courses has been extensively\r\nresearched. However, most of these studies have focused on Python. In this work, we examine the bug-fixing and feedback-generation\r\nabilities of Code Llama and ChatGPT for Java programming assignments using our new Java benchmark called CodeWBugs. The\r\nresults indicate that ChatGPT performs reasonably well, and was able to fix 94.33% programs. By comparison, we observed high\r\nvariability in the results from Code Llama. We further analyzed the impact of different types of prompts and observed that prompts\r\nthat included task descriptions and test inputs yielded better results. In most cases, the LLMs precisely localized the bugs and also\r\noffered guidance on how to proceed. Nevertheless, we also noticed incorrect responses generated by the LLMs, emphasizing the need\r\nto validate responses before disseminating feedback to learners.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "sd102@hw.ac.uk",
                "first": "Smitha",
                "last": "S Kumar",
                "affiliation": "Heriot Watt University -UAE",
                "contact": true
            },
            {
                "email": "m.lones@hw.ac.uk",
                "first": "Michael",
                "last": "Lones",
                "affiliation": "Heriot Watt University- UK",
                "contact": true
            },
            {
                "email": "m.maarek@hw.ac.uk",
                "first": "Manuel",
                "last": "Maarek",
                "affiliation": "Heriot-Watt University -UK",
                "contact": true
            },
            {
                "email": "h.zantout@hw.ac.uk",
                "first": "Hind",
                "last": "Zantout",
                "affiliation": "Heriot-Watt University -UAE",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701891254,
        "final_submitted": true,
        "final_submitted_at": 1706091002
    },
    {
        "pid": 21,
        "title": "Semantically Aligned Question and Code Generation for Automated Insight Generation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-46ae2d102040ec22c550887160617092c0ef793e65f3b900f584d39b34113ec7",
            "timestamp": 1702035202,
            "size": 1040002,
            "pages": 8
        },
        "abstract": "Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or \\emph{align}) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "ananyasingha2000@gmail.com",
                "first": "Ananya",
                "last": "Singha",
                "affiliation": "Microsoft",
                "contact": true
            },
            {
                "email": "t-bhchopra@microsoft.com",
                "first": "Bhavya",
                "last": "Chopra",
                "affiliation": "Microsoft",
                "contact": true
            },
            {
                "email": "t-anikhatry@microsoft.com",
                "first": "Anirudh",
                "last": "Khatry",
                "affiliation": "Microsoft"
            },
            {
                "email": "sumitg@microsoft.com",
                "first": "Sumit",
                "last": "Gulwani",
                "affiliation": "Microsoft"
            },
            {
                "email": "azh321@gmail.com",
                "first": "Austin",
                "last": "Henley",
                "affiliation": "Microsoft",
                "contact": true
            },
            {
                "email": "levu@microsoft.com",
                "first": "Vu",
                "last": "Le",
                "affiliation": "Microsoft"
            },
            {
                "email": "chrisparnin@microsoft.com",
                "first": "Chris",
                "last": "Parnin",
                "affiliation": "Microsoft",
                "contact": true
            },
            {
                "email": "singhmukul@microsoft.com",
                "first": "Mukul",
                "last": "Singh",
                "affiliation": "Microsoft"
            },
            {
                "email": "gverbruggen@microsoft.com",
                "first": "Gust",
                "last": "Verbruggen",
                "affiliation": "Microsoft",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "taoxie@pku.edu.cn": "collaborator",
            "bram.adams@queensu.ca": "collaborator",
            "nachiappan.nagappan@gmail.com": "institutional",
            "rayb@cs.columbia.edu": "collaborator",
            "tzimmer@microsoft.com": "institutional",
            "jlou@microsoft.com": "institutional"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701972163
    },
    {
        "pid": 32,
        "title": "LLM4TDD: Best Practices for Test Driven Development Using Large Language Models",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-e7f64764c9bdcb6b4fcc198b56314b4b022c2df4b156e674034fff8dc5448d7c",
            "timestamp": 1701980007,
            "size": 1069604,
            "pages": 8
        },
        "abstract": "In today's society, we are becoming increasingly dependent on software systems. However, we also constantly witness the negative impacts of buggy software. Program synthesis aims to improve software correctness by automatically generating the program given an outline of the expected behavior. For decades, program synthesis has been an active research field, with recent approaches looking to incorporate Large Language Models to help generate code. This paper explores the concept of LLM4TDD, where we guide Large Language Models to generate code iteratively using a test-driven development methodology. Specifically, our framework has the user write test code, but uses Large Language Models to generate the actual implementation. We conduct an empirical evaluation using ChatGPT and coding problems from LeetCode to investigate the impact of different test, prompt and problem attributes on the efficacy of LLM4TDD.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "sanyogita.piya@mavs.uta.edu",
                "first": "Sanyogita",
                "last": "Piya",
                "affiliation": "The University of Texas at Arlington",
                "contact": true
            },
            {
                "email": "allison.sullivan@uta.edu",
                "first": "Allison",
                "last": "Sullivan",
                "affiliation": "The University of Texas at Arlington",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701980007
    },
    {
        "pid": 38,
        "title": "Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-aefa2fd1878a3863d311b786f892b3303468e419c6a6ca2702620921bcd804ae",
            "timestamp": 1702048568,
            "size": 1067763,
            "pages": 8
        },
        "abstract": "Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3)  Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. \r\nOur results show that ChatGPT’s performance is comparable with Pynguin in terms of coverage, though for some cases, its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT’s performance, achieving a much higher coverage.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "shreya20542@iiitd.ac.in",
                "first": "Shreya",
                "last": "Bhatia",
                "affiliation": "IIIT Delhi",
                "contact": true
            },
            {
                "email": "tarushi20579@iiitd.ac.in",
                "first": "Tarushi",
                "last": "Gandhi",
                "affiliation": "IIIT Delhi",
                "contact": true
            },
            {
                "email": "dhruv.kumar@iiitd.ac.in",
                "first": "Dhruv",
                "last": "Kumar",
                "affiliation": "IIIT Delhi",
                "contact": true
            },
            {
                "email": "jalote@iiitd.ac.in",
                "first": "Pankaj",
                "last": "Jalote",
                "affiliation": "IIIT Delhi",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702036638,
        "final_submitted": true,
        "final_submitted_at": 1707830006
    },
    {
        "pid": 39,
        "title": "LLM-based and Retrieval-Augmented Control Code Generation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-2a4ff34a8eaae0a254061b5c3734cb43e295369347540a3bb5fe0573a06ffe61",
            "timestamp": 1702055175,
            "size": 2615366,
            "pages": 8
        },
        "abstract": "Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, OpenPLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "heiko.koziolek@de.abb.com",
                "first": "Heiko",
                "last": "Koziolek",
                "affiliation": "ABB Research",
                "contact": true
            },
            {
                "email": "sten.gruener@de.abb.com",
                "first": "Sten",
                "last": "Grüner",
                "affiliation": "ABB Research"
            },
            {
                "email": "rhaban.hark@de.abb.com",
                "first": "Rhaban",
                "last": "Hark",
                "affiliation": "ABB Research"
            },
            {
                "email": "virendra.ashiwal@de.abb.com",
                "first": "Virendra",
                "last": "Ashiwal",
                "affiliation": "ABB Research",
                "contact": true
            },
            {
                "email": "sofia.linsbauer@de.abb.com",
                "first": "Sofia",
                "last": "Linsbauer",
                "affiliation": "ABB Research"
            },
            {
                "email": "nafise.eskandani@de.abb.com",
                "first": "Nafise",
                "last": "Eskandani",
                "affiliation": "ABB Research",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702021997
    },
    {
        "pid": 41,
        "title": "LLM-based Control Code Generation using Image Recognition",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9389127549444fe0d35168b57b3d938446c179d015d3fb71f43f750f7b3efd72",
            "timestamp": 1702174412,
            "size": 3176739,
            "pages": 8
        },
        "abstract": "LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P&IDs) using image recognition. We have evaluated the method in three case study with industrial P&IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "heiko.koziolek@de.abb.com",
                "first": "Heiko",
                "last": "Koziolek",
                "affiliation": "ABB Corporate Research",
                "contact": true
            },
            {
                "email": "koziolek@kit.edu",
                "first": "Anne",
                "last": "Koziolek",
                "affiliation": "KIT Karlsruhe Institute of Technology"
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701768843
    },
    {
        "pid": 42,
        "title": "Benchmarking the Security Aspect of Large Language Model-Based Code Generation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-562ff163e38b0e01303cdcd867fcd5dd73eedbcf05328c69e9b1a040006dc1ee",
            "timestamp": 1702036400,
            "size": 596326,
            "pages": 4
        },
        "abstract": "Benchmark plays a pivotal role in advancing the research on the programming related tasks. In this study, we introduce, PyP4LLMSec, a Python benchmark designed to assess the security aspect of Python code generated by large language models (LLMs). Our methodology involves an analysis of Common Vulnerabilities and Exposures (CVEs) over the past two years. We identified 257 vulnerability-related commits associated with these CVEs across 143 open-source Python projects on GitHub. Subsequently, we conducted manual inspections of the vulnerable code, identifying and analyzing 295 code patches addressing vulnerabilities to generate Python code prompts at the file, class, and function granularity levels. As a result, we generated 2142 prompts with three distinct types of endings at various granularity levels, covering 15 different Common Weakness Enumeration (CWE) categories. To the best of our knowledge, this dataset represents the first collection of Python programming language prompts for scrutinizing the security of code generated by LLMs across different granularity levels. Our dataset, PyP4LLMSec, is publicly accessible on GitHub.",
        "paper_type": "2. Position Paper",
        "authors": [
            {
                "email": "cheng.cheng.20171@mail.concordia.ca",
                "first": "Cheng",
                "last": "Cheng",
                "affiliation": "Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada.",
                "contact": true
            },
            {
                "email": "jinqiuy@encs.concordia.ca",
                "first": "Jinqiu",
                "last": "Yang",
                "affiliation": "Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada.",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "lintan@purdue.edu": "other",
            "bram.adams@queensu.ca": "other",
            "shinhwei.tan@concordia.ca": "institutional"
        },
        "archival_option": "2. Non-archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702033451
    },
    {
        "pid": 43,
        "title": "HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-f67c843e1365c605db93799209cd0cac2bb3dc4b4df634adad1323b435a3ce72",
            "timestamp": 1701969533,
            "size": 811337,
            "pages": 8
        },
        "abstract": "Existing code summarization approaches primarily leverage Abstract Syntax Trees (ASTs) and sequential information from source code to generate code summaries while often overlooking the critical considerations of dependencies among code elements and code hierarchy. However, effective summarization necessitates a holistic analysis of code snippets from three distinct aspects: lexical, syntactic, and semantic information. Large language models recently have shown remarkable performance across various tasks, yet their capability of capturing features of code has not been officially demonstrated.  In this paper, we propose a novel code summarization approach utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs adeptly capture essential code features at lexical, syntactic, and semantic levels within a hierarchical structure. Our HierarchyNet processes each layer of the HCR separately, employing a Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. In addition, our approach demonstrates superior performance compared to fine-tuned pre-trained models, including CodeT5, and CodeBERT, as well as large language models that employ zero\/few-shot settings, such as StarCoder and CodeGen. Implementation details can be found at https:\/\/anonymous.4open.science\/r\/HierarchyNet-41FC.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "minh.nghminh@gmail.com",
                "first": "Minh",
                "last": "Nguyen",
                "affiliation": "FPT Software AI Center",
                "contact": true
            },
            {
                "email": "dqnbui.2016@smu.edu.sg",
                "first": "Nghi D. Q.",
                "last": "Bui",
                "affiliation": "Fulbright University, Viet Nam",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "2. Non-archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701969533,
        "final_submitted": true,
        "final_submitted_at": 1705594808
    },
    {
        "pid": 50,
        "title": "LLMs for Relational Reasoning: How Far are We?",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-e4b67125a7af43105b3a796bd06af9d553f0a6a94e0a1d7424692b4a7dd3362c",
            "timestamp": 1701966833,
            "size": 1166719,
            "pages": 8
        },
        "abstract": "Large language models (LLMs) have revolutionized many areas (e.g., natural language processing, software engineering, etc) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple in nature, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent effort has demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction\/synthesis systems as it requires inducing strict cause-effect logic so as to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "zhiming001@e.ntu.edu.sg",
                "first": "Zhiming",
                "last": "Li",
                "affiliation": "Nanyang Technological University",
                "contact": true
            },
            {
                "email": "yushi002@e.ntu.edu.sg",
                "first": "Yushi",
                "last": "Cao",
                "affiliation": "Nanyang Technological University",
                "contact": true
            },
            {
                "email": "xiufeng001@e.ntu.edu.sg",
                "first": "Xiufeng",
                "last": "Xu",
                "affiliation": "Nanyang Technological University",
                "contact": true
            },
            {
                "email": "junzhe.jiang@connect.polyu.hk",
                "first": "Junzhe",
                "last": "Jiang",
                "affiliation": "Hong Kong Polytechnic University",
                "contact": true
            },
            {
                "email": "liuxu@comp.nus.edu.sg",
                "first": "Xu",
                "last": "Liu",
                "affiliation": "National University of Singapore",
                "contact": true
            },
            {
                "email": "yon.shin.teo@continental-corporation.com",
                "first": "Yon Shin",
                "last": "Teo",
                "affiliation": "Continental Automotive Singapore Pte. Ltd.",
                "contact": true
            },
            {
                "email": "shang-wei.lin@ntu.edu.sg",
                "first": "Shang-Wei",
                "last": "Lin",
                "affiliation": "Nanyang Technological University",
                "contact": true
            },
            {
                "email": "yangliu@ntu.edu.sg",
                "first": "Yang",
                "last": "Liu",
                "affiliation": "Nanyang Technological University",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "scc@cse.ust.hk": "other",
            "yangliu@ntu.edu.sg": "advisor author",
            "jie.zhang@kcl.ac.uk": "other",
            "taoxie@pku.edu.cn": "personal",
            "pengxin@fudan.edu.cn": "other",
            "chong.wang@ntu.edu.sg": "institutional"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701851864
    },
    {
        "pid": 51,
        "title": "Translation of Low-Resource COBOL to Logically Correct and Readable Java leveraging High-Resource Java Refinement",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-33c87ff80036ad922ec71bc35421bd252f1a451307ca9f40b739d70b8a162786",
            "timestamp": 1701940616,
            "size": 3223761,
            "pages": 8
        },
        "abstract": "Automated translation of legacy code to modern programming languages is the need of the hour for modernizing enterprise systems. This work specifically addresses automated COBOL to Java translation. Traditional rule-based tools for this perform statement-wise translation, overlooking possible modularization and refactoring of the source COBOL code to translate to human-readable target Java code. Our investigation reveals that state-of-the-art Large Language Models (LLMs) in the domain of code encounter difficulties with regard to logical correctness and readability when directly translating low-resource COBOL code to Java. To address these challenges, we propose an LLM-based workflow, leveraging temperature sampling and refinement-based strategies, to not only ensure logical correctness of the translation but also maximize the readability of the target Java code. We exploit the fact that, due to their extensive exposure to human-written Java codes during pre-training, the LLMs are more equipped with profound comprehension and capability for refining translated Java codes than COBOL to Java translation. With a dataset sourced from CodeNet, we demonstrate that sequential refinement of the translated high-resource Java code with execution-guided logic feedback followed by LLM-based readability feedback, yields better performance in terms of logical correctness (81.99\\% execution accuracy) and readability (0.610 score), than  LLM based translation with test cases and readability guidance  (60.25\\% and 0.539) or refinement of the translation task itself (77.95\\% and 0.572).",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "gandhi.shubham@tcs.com",
                "first": "Shubham",
                "last": "Gandhi",
                "affiliation": "TCS Research",
                "contact": true
            },
            {
                "email": "manasi.patwardhan@tcs.com",
                "first": "Manasi",
                "last": "Patwardhan",
                "affiliation": "TCS Research",
                "contact": true
            },
            {
                "email": "jyotsana.khatri@tcs.com",
                "first": "Jyotsana",
                "last": "Khatri",
                "affiliation": "TCS Research",
                "contact": true
            },
            {
                "email": "lovekesh.vig@tcs.com",
                "first": "Lovekesh",
                "last": "Vig",
                "affiliation": "TCS Research, New Delhi, India"
            },
            {
                "email": "raveendra.kumar@tcs.com",
                "first": "Raveendra Kumar",
                "last": "Medicherla",
                "affiliation": "TCS Research",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701194614
    },
    {
        "pid": 63,
        "title": "Tackling Students' Coding Assignments with LLMs",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-730bc0fdf01360fd74c763ff9b187aa1c31033e01629cb93ec96b27837ff9606",
            "timestamp": 1702036006,
            "size": 638304,
            "pages": 8
        },
        "abstract": "State-of-the-art large language models (LLMs) have demonstrated an extraordinary ability to write computer code. This ability can be quite beneficial when integrated into an IDE to assist a programmer with basic coding. On the other hand, it may be misused by computer science students for cheating on coding tests or homework assignments. At present, knowledge about the exact capabilities and limitations of state-of-the-art LLMs is still inadequate. Furthermore, their capabilities have been changing quickly with each new release. In this paper, we present a dataset of 559 programming exercises in 10 programming languages collected from a system for evaluating coding assignments at our university. We have experimented with four well-known LLMs (GPT 3.5, GPT 4, Codey, Code Llama) and asked them to solve these assignments. The evaluation results are intriguing and provide insights into the strengths and weaknesses of the models, as well as the dangers and benefits that LLMs pose for computer science education.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "dingle@ksvi.mff.cuni.cz",
                "first": "Adam",
                "last": "Dingle",
                "affiliation": "Charles University",
                "contact": true
            },
            {
                "email": "krulis@d3s.mff.cuni.cz",
                "first": "Martin",
                "last": "Kruliš",
                "affiliation": "Charles University",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701991910
    },
    {
        "pid": 65,
        "title": "Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-19d9b1f620a269bf985426f8da1f432e1fe688f1d6dc40c0d3e22fd201deec40",
            "timestamp": 1702032285,
            "size": 1999428,
            "pages": 4
        },
        "abstract": "Automated debugging is an emerging research field that aims to automatically find and repair bugs.\r\nIn this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts.\r\nMost recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising.\r\nHowever, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT).\r\nIn this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0.\r\nWe select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J.\r\nFor FL and APR, we designed three kinds of prompts for each, considering different kinds of information.\r\nThe results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.",
        "paper_type": "2. Position Paper",
        "authors": [
            {
                "email": "21291232@bjtu.edu.cn",
                "first": "Shengbei",
                "last": "Jiang",
                "affiliation": "Beijing Jiaotong University"
            },
            {
                "email": "21281296@bjtu.edu.cn",
                "first": "Jiabao",
                "last": "Zhang",
                "affiliation": "Beijing Jiaotong University"
            },
            {
                "email": "21281275@bjtu.edu.cn",
                "first": "Wei",
                "last": "Chen",
                "affiliation": "Beijing Jiaotong University"
            },
            {
                "email": "wangbo_cs@bjtu.edu.cn",
                "first": "Bo",
                "last": "Wang",
                "affiliation": "Beijing Jiaotong University",
                "contact": true
            },
            {
                "email": "zhoujianyi2@huawei.com",
                "first": "Jianyi",
                "last": "Zhou",
                "affiliation": "Huawei Cloud Computing Technologies Co., Ltd."
            },
            {
                "email": "jie.zhang@kcl.ac.uk",
                "first": "Jie",
                "last": "Zhang",
                "affiliation": "King's College London",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "yilinglou@fudan.edu.cn": "collaborator",
            "mark.harman@ucl.ac.uk": "collaborator",
            "yangliu@ntu.edu.sg": "collaborator",
            "jie.zhang@kcl.ac.uk": "collaborator author",
            "michail.papadakis@uni.lu": "collaborator"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702033877
    },
    {
        "pid": 76,
        "title": "HawkEyes: Spotting and Evading Instruction Disalignments of LLMs",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-1850f5341fb2cfe805a4707e3b06f3292de2572bb1342569a0ee48f427d674c8",
            "timestamp": 1702026177,
            "size": 1035842,
            "pages": 7
        },
        "abstract": "LLM agents have been demonstrated to be powerful in vision-language planning (VLP) tasks. \r\nHowever, they often encounter challenges with sequential VLP tasks, particularly in adhering to instructions in prompts, which affects their overall efficacy.\r\nTo unleash the efficacy of LLM agents against instruction disalignments, this paper proposes HawkEyes, an LLM-based approach to self-identify and self-avoid instruction disalignments of any given LLM agent.\r\nInstead of altering the intrinsic mechanism of LLM agents, HawkEyes operates externally on the input and output sequences of LLM agents.\r\nSpecifically, HawkEyes uses LLMs to decompose the instructions in the LLM agent's workflow into primitive constraints, creating oracles to detect any disalignments of these primitive constraints and synthesize avoiding actions to preempt potential disalignments. \r\nThis paper also demonstrates the application of HawkEyes to enhance three state-of-the-art LLM agents, assessing HawkEyes's effectiveness on two challenging VLP tasks: WebShop and MoTIF.\r\nEvaluation results show that HawkEyes significantly boosts the performance of LLM agents across various agents and tasks.\r\nNotably, HawkEyes doubles the success rate of LLM-planner, a state-of-the-art LLM agent dedicated to sequential VLP, from 17.2\\% to 34.5\\% on the MoTIF dataset, showcasing its capability to adapt LLM planning more flexibly and effectively in sequential VLP scenarios.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "dezhiran@pku.edu.cn",
                "first": "Dezhi",
                "last": "Ran",
                "affiliation": "Peking University",
                "contact": true
            },
            {
                "email": "zihe.song@utdallas.edu",
                "first": "Zihe",
                "last": "Song",
                "affiliation": "The University of Texas at Dallas",
                "contact": true
            },
            {
                "email": "pku_zwh@pku.edu.cn",
                "first": "Wenhan",
                "last": "Zhang",
                "affiliation": "Peking University"
            },
            {
                "email": "wei.yang@utdallas.edu",
                "first": "Wei",
                "last": "Yang",
                "affiliation": "UT Dallas",
                "contact": true
            },
            {
                "email": "taoxie@pku.edu.cn",
                "first": "Tao",
                "last": "Xie",
                "affiliation": "Peking University",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "yilinglou@fudan.edu.cn": "collaborator",
            "lingming@illinois.edu": "collaborator",
            "scc@cse.ust.hk": "collaborator",
            "taoxie@pku.edu.cn": "author",
            "pengxin@fudan.edu.cn": "collaborator",
            "jlou@microsoft.com": "collaborator",
            "wei.yang@utdallas.edu": "author"
        },
        "archival_option": "2. Non-archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702026177
    },
    {
        "pid": 81,
        "title": "Toward a New Era of Rapid Development: Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-9ded54dfe0b6dd6b3f14851b0f73c5b033f09eb0ada4e163856255a12395d5af",
            "timestamp": 1702036724,
            "size": 433839,
            "pages": 4
        },
        "abstract": "The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes.\r\nThis paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files.\r\nIn our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams.\r\nWe used 3 different prompts for each input, and we manually evaluated the results.\r\nWe created a scoring system in which we scored the occurrence of elements found in the diagram within the source code.\r\nOn average, the model was able to generate source code for 88\\% of the elements shown in the diagrams.\r\nOur results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files.\r\nHowever, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams.\r\nIn summary, further investigations are necessary to exploit the model's potential completely.",
        "paper_type": "2. Position Paper",
        "authors": [
            {
                "email": "antal@inf.u-szeged.hu",
                "first": "Gábor",
                "last": "Antal",
                "affiliation": "Department of Software Engineering, University of Szeged, Hungary",
                "contact": true
            },
            {
                "email": "vozar.svab@gmail.com",
                "first": "Richárd",
                "last": "Vozár",
                "affiliation": "Department of Software Engineering, University of Szeged, Hungary"
            },
            {
                "email": "ferenc@inf.u-szeged.hu",
                "first": "Rudolf",
                "last": "Ferenc",
                "affiliation": "Department of Software Engineering, University of Szeged, Hungary"
            }
        ],
        "pc_conflicts": {
            "amesbah@ece.ubc.ca": "collaborator"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702036590
    },
    {
        "pid": 84,
        "title": "Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-ba32397f208b4d61b1ba205442e33522925df44d560a10c35c6589efa82c1a41",
            "timestamp": 1701853148,
            "size": 757899,
            "pages": 4
        },
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we find that Integrated Development Environments (IDEs) can provide direct, accurate and real time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of LLMs of repository-level code completion. We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration.",
        "paper_type": "2. Position Paper",
        "authors": [
            {
                "email": "ycli21@cse.cuhk.edu.hk",
                "first": "Yichen",
                "last": "Li",
                "affiliation": "The Chinese University of Hong Kong",
                "contact": true
            },
            {
                "email": "ypeng@cse.cuhk.edu.hk",
                "first": "Yun",
                "last": "Peng",
                "affiliation": "The Chinese University of Hong Kong",
                "contact": true
            },
            {
                "email": "ythuo@cse.cuhk.edu.hk",
                "first": "Yintong",
                "last": "Huo",
                "affiliation": "The Chinese University of Hong Kong",
                "contact": true
            },
            {
                "email": "lyu@cse.cuhk.edu.hk",
                "first": "Michael R.",
                "last": "Lyu",
                "affiliation": "The Chinese University of Hong Kong",
                "contact": true
            }
        ],
        "pc_conflicts": {
            "scc@cse.ust.hk": "collaborator",
            "yangliu@ntu.edu.sg": "personal",
            "taoxie@pku.edu.cn": "collaborator",
            "tianyi@purdue.edu": "collaborator",
            "pengxin@fudan.edu.cn": "personal",
            "jlou@microsoft.com": "collaborator"
        },
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701853148
    },
    {
        "pid": 89,
        "title": "Gauging Tech Community Acceptance of Rapid Prototyping in Unfamiliar Programming Languages using LLM Chatbots",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-3c8a97465ea34297a27177fc26a0f98d0e3923e0459ec3f281577580473fb204",
            "timestamp": 1701965992,
            "size": 692644,
            "pages": 6
        },
        "abstract": "Large Language Model (LLM) chatbots such as ChatGPT possess information not only about human languages but also computer languages. It is now possible to perform programming and software design tasks with assistance from ChatGPT. We are particularly interested in how the software development community views the use of LLM chatbots in rapid prototyping using unfamiliar programming languages. In four different tech events, several example scenarios of how a tech-savvy engineer could use ChatGPT to prototype apps in unfamiliar programming languages were demonstrated, including a health education app. The four events include an IEEE chapter workshop, an IEEE WIE (Woman In Engineering) meeting, an IEEE joint chapter talk, and a university-level Computer Science class. The responses from the tech audience showed that the majority perceived value in the use of LLM chatbots in these contexts, even though there were subtle differences among different groups. This shows the need for further research on how to effectively incorporate LLM chatbots into traditional software design workflow to better serve the software development community.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "kc555014@ohio.edu",
                "first": "Krerkkiat",
                "last": "Chusap",
                "affiliation": "Ohio University",
                "contact": true
            },
            {
                "email": "liuc@ohio.edu",
                "first": "Chang",
                "last": "Liu",
                "affiliation": "Ohio University",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701965992
    },
    {
        "pid": 95,
        "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-bf75304d562ef75cf4187153fa27c75aca6750a456f566bb03aab2c93d122c63",
            "timestamp": 1701878755,
            "size": 1120809,
            "pages": 18
        },
        "abstract": "Code LLMs have the potential to make it easier for non-experts\r\nto understand and write code. However, current CodeLLM bench-\r\nmarks rely on a single expert-written prompt per problem, making\r\nit hard to generalize their success to non-expert users. In this paper,\r\nwe present a new natural-language-to-code benchmark of prompts\r\nwritten by a key population of non-experts: beginning program-\r\nmers. StudentEval contains 1,749 prompts written by 80 students\r\nwho have only completed one introductory Python course. Studen-\r\ntEval contains numerous non-expert prompts describing the same\r\nproblem, enabling exploration of key factors in prompt success. We\r\nuse StudentEval to evaluate 12 Code LLMs and find that Studen-\r\ntEval is a better discriminator of model performance than existing\r\nbenchmarks. Our analysis of student prompting strategies reveals\r\nthat nondeterministic LLM sampling can mislead students about\r\nthe quality of their descriptions, a finding with key implications for\r\nCode LLMs in education.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "first": "Hannah McLean",
                "last": "Babe",
                "affiliation": "Oberlin College"
            },
            {
                "first": "Sydney",
                "last": "Nguyen",
                "affiliation": "Wellesley College"
            },
            {
                "email": "zi.ya@northeastern.edu",
                "first": "Yangtian",
                "last": "Zi",
                "affiliation": "Northeastern University"
            },
            {
                "email": "a.guha@northeastern.edu",
                "first": "Arjun",
                "last": "Guha",
                "affiliation": "Northeastern University and Roblox",
                "contact": true
            },
            {
                "email": "mfeldman@oberlin.edu",
                "first": "Molly Q",
                "last": "Feldman",
                "affiliation": "Oberlin College",
                "contact": true
            },
            {
                "email": "carolyn.anderson@wellesley.edu",
                "first": "Carolyn Jane",
                "last": "Anderson",
                "affiliation": "Wellesley College",
                "contact": true
            }
        ],
        "pc_conflicts": {},
        "archival_option": "2. Non-archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701878755
    },
    {
        "pid": 99,
        "title": "Learn to Code Sustainably: An Empirical Study on Green Code Generation",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-86eba3a5cb68559541ff67f5b011f0980cca74dac82c88c8c38c24e54b2be38d",
            "timestamp": 1701959904,
            "size": 1686345,
            "pages": 8
        },
        "abstract": "The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code’s “green capacity”, based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "tina.vartziotis@twt-gmbh.de",
                "first": "Tina",
                "last": "Vartziotis",
                "affiliation": "TWT Science and Innovation, National Technical University of Athens",
                "contact": true
            },
            {
                "email": "ippolyti@mit.edu",
                "first": "Ippolyti",
                "last": "Dellatolas",
                "affiliation": "Massachusetts Institute of Technology",
                "contact": true
            },
            {
                "email": "georgios_dasoulas@hms.harvard.edu",
                "first": "George",
                "last": "Dasoulas",
                "affiliation": "Harvard University",
                "contact": true
            },
            {
                "email": "maximilian.schmidt@twt-gmbh.de",
                "first": "Maximilian",
                "last": "Schmidt",
                "affiliation": "TWT Science and Innovation",
                "contact": true
            },
            {
                "email": "florian.schneider@twt-gmbh.de",
                "first": "Florian",
                "last": "Schneider",
                "affiliation": "TWT Science and Innovation"
            },
            {
                "email": "tim.t.hoffmann@mercedes-benz.com",
                "first": "Tim",
                "last": "Hoffmann",
                "affiliation": "Mercedes-Benz"
            },
            {
                "email": "skots@mit.edu",
                "first": "Sotirios",
                "last": "Kotsopoulos",
                "affiliation": "National Technical University of Athens, Massachusetts Institute of Technology"
            },
            {
                "email": "michael.keckeisen@twt-gmbh.de",
                "first": "Michael",
                "last": "Keckeisen",
                "affiliation": "TWT Science and Innovation"
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1701960047
    },
    {
        "pid": 103,
        "title": "PromptSet: A Programmer’s Prompting Dataset",
        "submission": {
            "mimetype": "application\/pdf",
            "hash": "sha2-290c3249f38e50c71669639bcab730d67dbfc556b1b0c1b7292d7119c5d87bac",
            "timestamp": 1702010430,
            "size": 1740327,
            "pages": 7
        },
        "abstract": "The rise of capabilities expressed by large language models has been quickly followed by the integration of the same complex systems into application level logic. Algorithms, programs, systems, and companies are built around structured prompting to black box models where the majority of the design and implementation lies in capturing and quantifying the `agent mode'. The standard way to shape a closed language model is to prime it for a specific task with a tailored prompt, often initially handwritten by a human. The textual prompts co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be. Unlike traditional code, we find that prompts do not receive effective static testing and linting to prevent runtime issues. In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer prompts used in open source Python programs. We perform analysis on this dataset and introduce the notion of a static linter for prompts. Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name \\texttt{pisterlabs\/promptset}.",
        "paper_type": "1. Research Paper",
        "authors": [
            {
                "email": "kaiser@pister.dev",
                "first": "Kaiser",
                "last": "Pister",
                "affiliation": "Univeristy of Wisconsin-Madison",
                "contact": true
            },
            {
                "email": "dhruba.j.paul@gmail.com",
                "first": "Dhruba Jyoti",
                "last": "Paul",
                "affiliation": "Univeristy of Wisconsin-Madison",
                "contact": true
            },
            {
                "email": "idjoshi@wisc.edu",
                "first": "Ishan",
                "last": "Joshi",
                "affiliation": "Univeristy of Wisconsin-Madison"
            },
            {
                "email": "pcbrophy@wisc.edu",
                "first": "Patrick",
                "last": "Brophy",
                "affiliation": "Univeristy of Wisconsin-Madison"
            }
        ],
        "pc_conflicts": {},
        "archival_option": "1. Archival",
        "decision": "Accepted",
        "status": "accept",
        "submitted": true,
        "submitted_at": 1702009404,
        "final_submitted": true,
        "final_submitted_at": 1706225570
    }
]
